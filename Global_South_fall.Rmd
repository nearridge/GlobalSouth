---
title: "Global_South_Fall"
author: "Neeraj Sharma"
date: "11/3/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r echo = FALSE}
# Relevant to data importation, structuring and visualization
library(tidyverse)
library(knitr)
library(readr)
library(here)

# Relevant to data formatting
library(lubridate)
library(countrycode)

# Relevant to text analysis
library(tidytext)
library(stringr)
library(SnowballC)
library(textclean)
library(sentimentr)
library(tidyr)

# Used for webscraping
library(rvest)

# Andres' fancy package for text analysis
library(cleanNLP)
reticulate::use_python("/anaconda3/bin/python.app")
# I'm running the spacy implimentation of the NLP backend. I'm not sure exactly what this means. 
cnlp_init_spacy()
```

# Setup

Step 1 is to import the corpus and annotate the entire dataframe. Then I extract each the tokens dataframe and initialize the stopwords dictionary. Annotating the entire corpus is a deathwish at this point because it'll be super slow. I've chosen to reduce my corpus to being just from 1970 to 1979, at least at this early stage of the project. That gives me roughly 5 million words. 

```{r cache=TRUE}
# Load in the df of all the speeches by speech
imported_files <- read_tsv(here::here("Data", "raw_speeches_mikhaylov_project.tsv"))

# Annotate the structure of the 1970s decade speeches. 
annotated_imported_files_1970 <- cnlp_annotate(imported_files %>% filter(Year %in% (1970:1979)), as_strings = TRUE)

# Get the individual words for 1970s
all_words_only_1970 <- cnlp_get_token(annotated_imported_files_1970) %>%
  filter(!str_detect(word, "^[0-9]*$")) %>%
  filter(!upos == "PUNCT" & !upos == "SPACE") %>%
  mutate(lemma = strip(lemma)) %>%
  # correcting lemma development to be develop
  mutate(lemma = if_else(lemma == "development", "develop", lemma)) %>%
  mutate(id = str_sub(id, end=-5)) %>%
  separate(id, c("Country", "Session", "Year"), sep = "_", remove = FALSE)


annotated_imported_files_2k <- cnlp_annotate(imported_files %>% filter(Year %in% (2000:2009)), as_strings = TRUE)

all_words_only_2k <- cnlp_get_token(annotated_imported_files_2k) %>%
  filter(!str_detect(word, "^[0-9]*$")) %>%
  filter(!upos == "PUNCT" & !upos == "SPACE") %>%
  mutate(lemma = strip(lemma)) %>%
  #stemming development to distance_between_develop_other_words
  mutate(lemma = if_else(lemma == "development", "develop", lemma))

data(stop_words)
```


```{r}
annotated_imported_speeches <- read_tsv("../../../../../Desktop/full_un_corpus_annotated.tsv") %>%
  filter(!str_detect(word, "^[0-9]*$")) %>%
  filter(!upos == "PUNCT" & !upos == "SPACE") %>%
  mutate(lemma = strip(lemma)) %>%
  # correcting lemma development to be develop
  mutate(lemma = if_else(lemma == "development", "develop", lemma)) %>%
  mutate(lemma = if_else(lemma == "government", "govern", lemma)) %>%
  mutate(id = str_sub(id, end=-5)) %>%
  separate(id, c("Country", "Session", "Year"), sep = "_", remove = FALSE)
all_words_only <- annotated_imported_speeches
all_words_only_1970 <- all_words_only %>% filter(Year %in% (1970:1979))
all_words_only_1980 <- all_words_only %>% filter(Year %in% (1980:1989))
all_words_only_1990 <- all_words_only %>% filter(Year %in% (1990:1999))
all_words_only_2k <- all_words_only %>% filter(Year %in% (2000:2009))
all_words_only_2010 <- all_words_only %>% filter(Year %in% (2010:2018))

```

To import the 1970 and all time stuff so i can skip annotation each time. 
all_words_only_1970 <- read_tsv("../../../../../Desktop/all_words_only_1970.tsv")
all_words_only <- read_tsv("../../../../../Desktop/full_un_corpus_annotated.tsv") %>% filter(!str_detect(word, "^[0-9]*$")) %>%
  filter(!upos == "PUNCT" & !upos == "SPACE") %>%
  mutate(lemma = strip(lemma)) %>%
  # correcting lemma development to be develop
  mutate(lemma = if_else(lemma == "development", "develop", lemma)) %>%
  mutate(id = str_sub(id, end=-5)) %>%
  separate(id, c("Country", "Session", "Year"), sep = "_", remove = FALSE)
  
  
The words we are most interested at the moment are:

* Govern-
* Develop-
* Secur-

and any relevant derivatives of them. These are words that we'll tag the entire sentence of when they appear in a speech. 

# Function to determine the distance between words

This is a function that finds out the distance between two words in a string. It is my baby. It's important because my (read: Neeraj's possibly misinformed) hypothesis is that words relevant to a keyword will be said closer in a sentence to the keyword itself. At minimum, important words will be mentioned a lot. 

```{r}
# Builds a function that counts how far apart two words are
distance_between_words <- function(df_of_all_tokens, keyword) {
  df <- df_of_all_tokens %>% 
    filter(lemma == keyword) %>%
    select(id, sid) %>%
    mutate(sentence_number = row_number()) %>%
    inner_join(df_of_all_tokens, by = NULL) %>%
    select(id, sid, word, lemma, sentence_number)
  
  # number of sentences that say the keyword
  x <- max(select(df, sentence_number))
  
  datalist = list()
  for (i in 1:x) {
      # isolates gets the ith sentence that mentions the keyword
      limited_df <- filter(df, sentence_number == i)
      
      # Finds the location of the keyword in the sentence
      location_of_keyword <- limited_df %>%
          mutate(keyword_row_number = if_else(lemma == keyword, location <- as.double(row_number()), 0)) %>%
          filter(lemma == keyword) %>%
         select(sentence_number, keyword_row_number)
      
      final <- left_join(limited_df, location_of_keyword) %>%
        mutate(transient_row_number = as.double(row_number())) %>%
        mutate(length_of_sentence = max(transient_row_number)) %>%
        mutate(distance_from_keyword = abs(keyword_row_number - transient_row_number)) %>%
        mutate(distance_from_keyword_percent = distance_from_keyword/length_of_sentence)
      
      #appends all the ith sentences together into a large df
      datalist[[i]] <- final
}
  export <- bind_rows(datalist)

  return(export)
}
```

# Mentions of Govern with related words form the 1970 to 1979 corpus

```{r}
distance_between_govern_other_words <- distance_between_words(all_words_only_1970, "govern") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(27:n()) %>%
  arrange(-count) %>%
  filter(lemma != "govern")

ggplot(distance_between_govern_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Govern) versus mentions")
  # related words: scatter plot with count of total times said on x axis and distance from govern on y axis

# for some reason, the sentence division within the annotated_imported_files object is empty so cnlp_get_sentences returns an empty dataframe. What do I need to configure differently to get a) sentences to be parsed by the cnlp_annotate function up here and b) extract actual sentences and any information at that level.

kable(distance_between_govern_other_words %>% slice(1:15))

```


# Mentions of Secure with related words form the 1970 to 1979 corpus

```{r}
distance_between_secure_other_words <- distance_between_words(all_words_only_1970, "secure") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(24:n()) %>%
  arrange(-count) %>%
  filter(lemma != "secure")

ggplot(distance_between_secure_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Secure) versus mentions")

kable(distance_between_secure_other_words %>% slice(1:15))
```

# Mentions of Develop with related words form the 1970 to 1979 corpus

```{r}
distance_between_develop_other_words <- distance_between_words(all_words_only_1970, "develop") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(215:n()) %>%
  arrange(-count) %>%
  filter(lemma != "develop")

       fggplot(distance_between_develop_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions")

# that thing sitting all alone at 21 k is "country" unsurprisingly

ggplot(distance_between_develop_other_words %>% filter(lemma != "country"), mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions (minus country)")


kable(distance_between_develop_other_words %>% slice(1:15))
```

We had previously only looked at 1970 to 1979. Develop is super meaty, so we'll try to do some analysis on stuff way after 1979. We should expect to see some significant differences in the language of say, the 2000s compared to the language of the 1970s. 

Lets get an annotated corpus over 2000 to 2009. 

ITS GONE NOW AND HAS MOVED WAY UP TO THE START SETUP

Now lets look at develop and compare it to 1970s stuff. Especially in the context of a specific nation, say Indonesia. 

```{r}
distance_between_develop_other_words_2k <- distance_between_words(all_words_only_2k, "develop") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(87:n()) %>%
  arrange(-count) %>%
  filter(lemma != "develop")

ggplot(distance_between_develop_other_words_2k, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions in 2000s")

# that thing sitting all alone at 8.8 k is "country" unsurprisingly

ggplot(distance_between_develop_other_words_2k %>% filter(lemma != "country"), mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions (minus country) in 2000s")


kable(distance_between_develop_other_words_2k %>% slice(1:15))
```

# TFIDF

```{r}
#List of countries for filtering
country_filter <- str_to_lower(pull(codelist, country.name.en)) 

# I DONT WANT TO STEM RIGHT NOW. THAT MEANS RIGHT AND RIGHTS ARE DIFFERENT THINGS
# Brute force adds stemming but this is kinda unnecessary because lemma works well enough, at least so far.
#all_words_only <- all_words_only %>%
#  mutate(word_stem = wordStem(lemma))

# Function that gets the top tfidf words for each year in a decade. This is applied to a dataframe that already has done tfidf analyis on a bunch of words. 
tfidf_keyword_decades <- function(df_of_tfidf_of_every_individual_word, decade) {
  df_of_tfidf_of_every_individual_word <- df_of_tfidf_of_every_individual_word %>% 
  separate(id, c("Country", "Session", "Year")) %>%
  filter(Year %in% seq(decade, decade+9)) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "develop" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10) 
  return(df_of_tfidf_of_every_individual_word)
}
```

TFIDF of the most unique words said at the UN ever. This is not insightful. 

```{r}
seperate_tfidf <- left_join(all_words_only %>% count(id, lemma), all_words_only %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))

full_tfidf <- seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  filter(tf_idf > 0.005)

full_tfidf
```

TFIDF for stuff related to govern

```{r}
sentences_mention_govern <- distance_between_words(all_words_only, "govern")
num_words_in_each_speech_mentioning_govern <- sentences_mention_govern %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n))

mentions_of_aux_words_in_sentences_containing_govern <- 
```

```{r}
#these are unstemmed
sentences_mention_govern_1970 <- distance_between_words(all_words_only_1970, "govern")
sentences_mention_govern_1980 <- distance_between_words(all_words_only_1980, "govern")
sentences_mention_govern_1990 <- distance_between_words(all_words_only_1990, "govern")
sentences_mention_govern_2k <- distance_between_words(all_words_only_2k, "govern")
sentences_mention_govern_2010 <- distance_between_words(all_words_only_2010, "govern")


govern_1970_seperate_tfidf <- left_join(sentences_mention_govern_1970 %>% count(id, lemma), sentences_mention_govern_1970 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
govern_1980_seperate_tfidf <- left_join(sentences_mention_govern_1980 %>% count(id, lemma), sentences_mention_govern_1980 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
govern_1990_seperate_tfidf <- left_join(sentences_mention_govern_1990 %>% count(id, lemma), sentences_mention_govern_1990 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
govern_2k_seperate_tfidf <- left_join(sentences_mention_govern_2k %>% count(id, lemma), sentences_mention_govern_2k %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
govern_2010_seperate_tfidf <- left_join(sentences_mention_govern_2010 %>% count(id, lemma), sentences_mention_govern_2010 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))

govern_full_tfidf_1970_ided <- govern_1970_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
govern_full_tfidf_1980_ided <- govern_1980_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
govern_full_tfidf_1990_ided <- govern_1990_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
govern_full_tfidf_2k_ided <- govern_2k_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
govern_full_tfidf_2010_ided <- govern_2010_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))

govern_full_tfidf_1970_yeared <- govern_full_tfidf_1970_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "govern" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

govern_full_tfidf_1980_yeared <- govern_full_tfidf_1980_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "govern" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

govern_full_tfidf_1990_yeared <- govern_full_tfidf_1990_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "govern" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

govern_full_tfidf_2k_yeared <- govern_full_tfidf_2k_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "govern" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

govern_full_tfidf_2010_yeared <- govern_full_tfidf_2010_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "govern" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10) 

govern_full_tfidf_1970_yeared
govern_full_tfidf_1980_yeared
govern_full_tfidf_1990_yeared
govern_full_tfidf_2k_yeared
govern_full_tfidf_2010_yeared
```

Develop

```{r}
sentences_mention_develop_1970 <- distance_between_words(all_words_only_1970, "develop")
sentences_mention_develop_1980 <- distance_between_words(all_words_only_1980, "develop")
sentences_mention_develop_1990 <- distance_between_words(all_words_only_1990, "develop")
sentences_mention_develop_2k <- distance_between_words(all_words_only_2k, "develop")
sentences_mention_develop_2010 <- distance_between_words(all_words_only_2010, "develop")


develop_1970_seperate_tfidf <- left_join(sentences_mention_develop_1970 %>% count(id, lemma), sentences_mention_develop_1970 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
develop_1980_seperate_tfidf <- left_join(sentences_mention_develop_1980 %>% count(id, lemma), sentences_mention_develop_1980 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
develop_1990_seperate_tfidf <- left_join(sentences_mention_develop_1990 %>% count(id, lemma), sentences_mention_develop_1990 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
develop_2k_seperate_tfidf <- left_join(sentences_mention_develop_2k %>% count(id, lemma), sentences_mention_develop_2k %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))
develop_2010_seperate_tfidf <- left_join(sentences_mention_develop_2010 %>% count(id, lemma), sentences_mention_develop_2010 %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))

develop_full_tfidf_1970_ided <- develop_1970_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
develop_full_tfidf_1980_ided <- develop_1980_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
develop_full_tfidf_1990_ided <- develop_1990_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
develop_full_tfidf_2k_ided <- develop_2k_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))
develop_full_tfidf_2010_ided <- develop_2010_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))

develop_full_tfidf_1970_yeared <- develop_full_tfidf_1970_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "develop" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

develop_full_tfidf_1980_yeared <- develop_full_tfidf_1980_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "develop" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

develop_full_tfidf_1990_yeared <- develop_full_tfidf_1990_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "develop" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

develop_full_tfidf_2k_yeared <- develop_full_tfidf_2k_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "develop" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10)

develop_full_tfidf_2010_yeared <- develop_full_tfidf_2010_ided %>%
  separate(id, c("Country", "Session", "Year")) %>%
  group_by(Year, lemma) %>%
  summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
  filter(n >= 5) %>%
  ungroup() %>%
  mutate(ratio = mean_tfidf/n) %>%
  arrange(desc(ratio)) %>%
  filter(lemma != "develop" 
         & lemma != "pron" 
         & lemma != "people" 
         & lemma != ""
         & lemma != "country"
         & lemma != "countries"
         & lemma != ""
         & lemma != "united"
         & lemma != "nation"
         & lemma != "nations"
         & lemma != "international"
         & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10) 

develop_full_tfidf_1970_yeared
develop_full_tfidf_1980_yeared
develop_full_tfidf_1990_yeared
develop_full_tfidf_2k_yeared
develop_full_tfidf_2010_yeared

# develop_full_tfidf_1970_yeared %>%
#  arrange(-desc(Year))
```

```{r}
sentences_mention_rights <- distance_between_words(all_words_only, "right")

right_all_seperate_tfidf <- left_join(sentences_mention_rights %>% count(id, lemma), sentences_mention_rights %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))

right_full_tfidf_all_ided <- right_all_seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  anti_join(stop_words, by = c("lemma" = "word"))


```


```{r}
rerun the big chunk above for rights and capibailities
adjust the words from funciton to look at sentence before and after


```


Do this but only for nouns. + verbs + adjs in 1970 and 2000s. Do it for these three words for a total of 18 tables. 1970s and 2000s, nouns verbs adjs for * Govern-
* Develop-
* Secur-
