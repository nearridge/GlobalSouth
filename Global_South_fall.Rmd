---
title: "Global_South_Fall"
author: "Neeraj Sharma"
date: "11/3/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r echo = FALSE}
# Relevant to data importation, structuring and visualization
library(tidyverse)
library(knitr)
library(readr)
library(here)

# Relevant to data formatting
library(lubridate)
library(countrycode)

# Relevant to text analysis
library(tidytext)
library(stringr)
library(SnowballC)
library(textclean)
library(sentimentr)
library(tidyr)

# Used for webscraping
library(rvest)

# Andres' fancy package for text analysis
library(cleanNLP)
reticulate::use_python("/anaconda3/bin/python.app")
# I'm running the spacy implimentation of the NLP backend. I'm not sure exactly what this means. 
cnlp_init_spacy()
```

# Setup

Step 1 is to import the corpus that I previouslly annotated of every UNGA speech. That was using corpusmaker.r. I store it on my desktop, and a version can also be found on UChicago box. 

```{r cache = TRUE}
annotated_imported_speeches <- read_tsv("../../../../../Desktop/full_un_corpus_annotated.tsv") %>%
  filter(!str_detect(word, "^[0-9]*$")) %>%
  filter(!upos == "PUNCT" & !upos == "SPACE") %>%
  mutate(lemma = strip(lemma)) %>%
  # correcting lemmas of some weird special cases. 
  # One day I might wordStem the entire thing. 
  mutate(lemma = if_else(lemma == "development", "develop", lemma)) %>%
  mutate(lemma = if_else(lemma == "government", "govern", lemma)) %>%
  mutate(id = str_sub(id, end=-5)) %>%
  separate(id, c("Country", "Session", "Year"), sep = "_", remove = FALSE)
all_words_only <- annotated_imported_speeches
all_words_only_1970 <- all_words_only %>% filter(Year %in% (1970:1979))
all_words_only_2k <- all_words_only %>% filter(Year %in% (2000:2009))
```

# Function to determine the distance between words

This is a function that finds out the distance between two words in a string. It is my baby. It's important because my (read: Neeraj's possibly misinformed) hypothesis is that words relevant to a keyword will be said closer in a sentence to the keyword itself. At minimum, important words will be mentioned a lot. 

```{r}
# returns full tidytext df of sentences containing a specific keyword
sentences_with_keyword <- function(df, keyword) {
  returner <- df %>%
    filter(lemma == keyword) %>% 
    select(id, sid) %>% 
    distinct() %>% 
    left_join(df)
  return(returner)
}

# This grabs the sentence in front of, the sentence itself, and the sentence behind the keyword. 
local_sentences_with_keyword <- function(df, keyword) {
  sentence_ids <- df %>%
    filter(lemma == keyword) %>% 
    select(id, sid) %>% 
    distinct()
  all_ids_sids <- all_words_only %>% 
    select(id, sid) %>% 
    distinct() 
  #need to lag it to the sid before and lead it to the sid after. Can't just use +1 and -1 because of weirdness in how data got processed. 
  before_middle_after_sentence_ids <- bind_rows(sentence_ids, sentence_ids %>% mutate(sid = sid - 1), sentence_ids %>% mutate(sid = sid + 1)) %>%
    distinct() %>%
    arrange(id, sid)
  returner <- semi_join(df, before_middle_after_sentence_ids) %>%
    arrange(id, sid)
  return(returner)
}

# Builds a function that counts how far apart two words are.
# the df will almost allways be all_words_only
distance_between_words <- function(df, keyword) {
  #begin Old sentences_with_keyword
  # This grabs all sentences with the keyword. It produces duplicates if the keyword appears multiple times. Those are removed later on. 
  df_of_location_sentences_with_keyword <- df %>% 
    filter(lemma == keyword) %>% 
    select(id, sid) %>% 
    mutate(sentence_number = row_number()) %>%
    unite(cat, c(id, sid), remove = FALSE) %>% 
    mutate(is_duplicate = duplicated(cat)) %>% 
    mutate(is_duplicate = if_else(is_duplicate == TRUE, TRUE, duplicated(cat, fromLast = TRUE)))
  df_of_all_sentences_with_keyword <- inner_join(df, df_of_location_sentences_with_keyword) %>% select(id, sid, cat, tid, word, lemma, sentence_number, is_duplicate)
  # Begin distance_between_words
  sentences_containing_keyword <- df_of_all_sentences_with_keyword
  
  location_of_keywords_in_each_sentence <- sentences_containing_keyword %>%
    mutate(keyword_row_number = if_else(lemma == keyword, location <- tid, 0)) %>%
    filter(lemma == keyword) %>%
    select(sentence_number, keyword_row_number)
  export <- inner_join(sentences_containing_keyword, location_of_keywords_in_each_sentence, by = ("sentence_number")) %>%
    unite(cat, c(cat, tid), remove = FALSE) %>%
    mutate(distance_between_word_and_keyword = abs(keyword_row_number - tid)) %>%
    mutate(compare_value = if_else(is_duplicate, lead(distance_between_word_and_keyword), 0)) %>%
    distinct(cat, .keep_all = TRUE) %>%
    mutate(distance_between_word_and_keyword = if_else(is_duplicate, pmin(distance_between_word_and_keyword, compare_value), distance_between_word_and_keyword))
  return(export)
}
```

When we first started out in the Fall, we thought that the words we were most interested in were:

* Govern-
* Develop-
* Secur-

and any relevant derivatives of them. Those are words that I tagged when they appeared in a speech. I need to update the next three sections of code to reflect the current function that gets distances between words and abilities to do the entire timespan of the corpus; not just 1970 to 1979. 

# Mentions of Govern with related words form the 1970 to 1979 corpus

```{r}
distance_between_govern_other_words <- distance_between_words(all_words_only_1970, "govern") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(27:n()) %>%
  arrange(-count) %>%
  filter(lemma != "govern")

ggplot(distance_between_govern_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Govern) versus mentions")
# related words: scatter plot with count of total times said on x axis and distance from govern on y axis

# for some reason, the sentence division within the annotated_imported_files object is empty so cnlp_get_sentences returns an empty dataframe. What do I need to configure differently to get a) sentences to be parsed by the cnlp_annotate function up here and b) extract actual sentences and any information at that level.

kable(distance_between_govern_other_words %>% slice(1:15))

```


# Mentions of Secure with related words form the 1970 to 1979 corpus

```{r}
distance_between_secure_other_words <- distance_between_words(all_words_only_1970, "secure") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(24:n()) %>%
  arrange(-count) %>%
  filter(lemma != "secure")

ggplot(distance_between_secure_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Secure) versus mentions")

kable(distance_between_secure_other_words %>% slice(1:15))
```

# Mentions of Develop with related words form the 1970 to 1979 corpus

```{r}
distance_between_develop_other_words <- distance_between_words(all_words_only_1970, "develop") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(215:n()) %>%
  arrange(-count) %>%
  filter(lemma != "develop")

fggplot(distance_between_develop_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions")

# that thing sitting all alone at 21 k is "country" unsurprisingly

ggplot(distance_between_develop_other_words %>% filter(lemma != "country"), mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions (minus country)")


kable(distance_between_develop_other_words %>% slice(1:15))
```

We had previously only looked at 1970 to 1979. Develop is super meaty, so we'll try to do some analysis on stuff way after 1979. We should expect to see some significant differences in the language of say, the 2000s compared to the language of the 1970s. 

Lets get an annotated corpus over 2000 to 2009. 

ITS GONE NOW AND HAS MOVED WAY UP TO THE START SETUP

Now lets look at develop and compare it to 1970s stuff. Especially in the context of a specific nation, say Indonesia. 

```{r}
distance_between_develop_other_words_2k <- distance_between_words(all_words_only_2k, "develop") %>%
  select(id, sid, word, lemma, length_of_sentence, distance_from_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(87:n()) %>%
  arrange(-count) %>%
  filter(lemma != "develop")

ggplot(distance_between_develop_other_words_2k, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions in 2000s")

# that thing sitting all alone at 8.8 k is "country" unsurprisingly

ggplot(distance_between_develop_other_words_2k %>% filter(lemma != "country"), mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions (minus country) in 2000s")


kable(distance_between_develop_other_words_2k %>% slice(1:15))
```


# TFIDF

The most recent breakthrough has been on TFIDF. 

```{r}
#List of countries for filtering
country_filter <- str_to_lower(pull(codelist, country.name.en)) 

# I DONT WANT TO STEM RIGHT NOW. THAT MEANS RIGHT AND RIGHTS ARE DIFFERENT THINGS
# Brute force adds stemming but this is kinda unnecessary because lemma works well enough, at least so far.
#all_words_only <- all_words_only %>%
#  mutate(word_stem = wordStem(lemma))

# Takes in a df of the words that appear in a sentence containing a keyword and returns a df of words that appear within a certain range of years. 
# Common ranges of years: a decade, each year, five years. 
# Example question this function answers: What were the most important words to govern in a 5 year period?
tfidfer <- function(df, start_year, up_to) {
  tfidf_over_range <- df %>%
    filter(Year %in% seq(start_year, start_year + up_to)) %>%
    bind_tf_idf(lemma, id, n) %>%
    filter(lemma %in% country_filter == FALSE) %>%
    anti_join(stop_words, by = c("lemma" = "word"))
  return(tfidf_over_range)
}

# Function that gets the top tfidf words in each year. This is applied to a dataframe that already has done tfidf analyis on a bunch of words related to a keyword over a period of time.
# The keyword technically does not matter for this function. 
tfidf_aux_word_organizer <- function(df_of_tfidf_of_every_individual_word) {
  df_of_tfidf_of_every_individual_word <- df_of_tfidf_of_every_individual_word %>% 
    separate(id, c("Country", "Session", "Year")) %>%
    group_by(Year, lemma) %>%
    summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
    filter(n >= 5) %>%
    ungroup() %>%
    mutate(ratio = mean_tfidf/n) %>%
    arrange(desc(ratio)) %>%
    filter(lemma != "develop" 
           & lemma != "pron" 
           & lemma != "people" 
           & lemma != ""
           & lemma != "country"
           & lemma != "countries"
           & lemma != ""
           & lemma != "united"
           & lemma != "nation"
           & lemma != "nations"
           & lemma != "international"
           & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10) 
  return(df_of_tfidf_of_every_individual_word)
}

printer_producer <- function(dataframe) {
  dataframe <- dataframe %>% 
    arrange(-desc(Year))
  data <- list()
  for (i in seq(as.double(min(dataframe$Year)), as.double(max(dataframe$Year)))) {
    data <- bind_cols(data, dataframe %>% filter(Year == i) %>% add_column(NA))
  }
  return(data)
}
```

TFIDF of the most unique words said at the UN ever. This is not insightful. 

```{r}
seperate_tfidf <- left_join(all_words_only %>% count(id, lemma), all_words_only %>% count(id, lemma) %>% group_by(id) %>% summarize(`words in speech` = sum(n)))

full_tfidf <- seperate_tfidf %>%
  bind_tf_idf(lemma, id, n) %>%
  filter(lemma %in% country_filter == FALSE) %>%
  arrange(desc(tf_idf)) %>%
  filter(tf_idf > 0.005)

full_tfidf
```

TFIDF for stuff related to govern. 

```{r}
# note that this produces an unstemmed all_words_only. 
sentences_mention_govern <- sentences_with_keyword(all_words_only, "govern")
num_words_in_each_speech_mentioning_govern <- sentences_mention_govern %>% 
  count(id, lemma) %>% 
  group_by(id) %>%
  summarize(`words in speech` = sum(n))

mentions_of_aux_words_in_sentences_containing_govern <- left_join(sentences_mention_govern %>% count(id, lemma), num_words_in_each_speech_mentioning_govern) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)

# tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1970, 9) %>% select(-Year, -Session, -Country) is equal to govern_full_tfidf_1970_ided
# furthermore, all of the govern_xxxxs equals the previous govern_full_tfidf_xxxx_yeared I previously had. Thus, it is the proper implimentation. 
govern_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1970, 9) %>% select(-Year, -Session, -Country))
govern_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1980, 9) %>% select(-Year, -Session, -Country))
govern_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1990, 9) %>% select(-Year, -Session, -Country))
govern_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 2000, 9) %>% select(-Year, -Session, -Country))
govern_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 2010, 9) %>% select(-Year, -Session, -Country))
```

```{r}
local_sentences_mention_govern <- local_sentences_with_keyword(all_words_only, "govern")
```

TFIDF for stuff related to Develop

```{r}
# note that this produces an unstemmed all_words_only. 
sentences_mention_develop <- sentences_with_keyword(all_words_only, "develop")
num_words_in_each_speech_mentioning_develop <- sentences_mention_develop %>% 
  count(id, lemma) %>% 
  group_by(id) %>% 
  summarize(`words in speech` = sum(n))

mentions_of_aux_words_in_sentences_containing_develop <- left_join(sentences_mention_develop %>% count(id, lemma), num_words_in_each_speech_mentioning_develop) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)

develop_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 1970, 9) %>% select(-Year, -Session, -Country))
develop_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 1980, 9) %>% select(-Year, -Session, -Country))
develop_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 1990, 9) %>% select(-Year, -Session, -Country))
develop_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 2000, 9) %>% select(-Year, -Session, -Country))
develop_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 2010, 9) %>% select(-Year, -Session, -Country))
```

TFIDF for stuff related to Rights

```{r}
# "rights" gets lemma'ed to "right"
# "Rights" gets lemma'ed to "rights." I do some overriding to get them to be the same in this context.
sentences_mention_right <- sentences_with_keyword(all_words_only %>% mutate(lemma = if_else(lemma == "rights", "right", lemma)), "right")
num_words_in_each_speech_mentioning_right <- sentences_mention_right %>%  
  count(id, lemma) %>% 
  group_by(id) %>% 
  summarize(`words in speech` = sum(n))

mentions_of_aux_words_in_sentences_containing_right <- left_join(sentences_mention_right %>% count(id, lemma), num_words_in_each_speech_mentioning_right) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)

right_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 1970, 9) %>% select(-Year, -Session, -Country))
right_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 1980, 9) %>% select(-Year, -Session, -Country))
right_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 1990, 9) %>% select(-Year, -Session, -Country))
right_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 2000, 9) %>% select(-Year, -Session, -Country))
right_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 2010, 9) %>% select(-Year, -Session, -Country))
```


rerun the big chunk above for rights and capibailities
adjust the words from funciton to look at sentence before and after especially for govern_1970s

TO DO TOMORROW
apply before and after sentences and see what emerges for govern especially
do capibalities to get a vocab cluster
do rights to find out specifically what rights
do part of speech tagging. 


Do this but only for nouns. + verbs + adjs in 1970 and 2000s. Do it for these three words for a total of 18 tables. 1970s and 2000s, nouns verbs adjs for * Govern-
* Develop-
* Secur-
