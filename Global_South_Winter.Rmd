---
title: "Global_South_Winter"
author: "Neeraj Sharma"
date: "3/2/2020"
output: github_document
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

Step one is the set up the environment. A number of packages are necessary for this analysis. 

```{r message = FALSE, warning = FALSE}
# Relevant to data importation, structuring and visualization
library(tidyverse)
library(knitr)
library(readr)
library(here)
library(excelR)

# Relevant to data formatting
library(lubridate)
library(countrycode)

# Relevant to text analysis
library(tidytext)
library(stringr)
library(SnowballC)
library(textclean)
library(sentimentr)
library(tidyr)

# Used for webscraping. This created the global north/south divide exploration that I did in Fall 2019. 
library(rvest)

# Andres' fancy package for text analysis
library(reticulate)
library(cleanNLP)
reticulate::use_python("/Users/neerajsharma/opt/anaconda3/bin/python.app", required = TRUE)
# I'm running the spacy implimentation of the NLP backend. I'm not sure exactly what this means. 
cnlp_init_spacy()
```

This shows the python implimentation this project is run off of, as well as the location of the cleanNLP package within this version of Anaconda Python. On Feburary 26, 2020, I updated my Macbook Pro from macOS Mojave to macOS Catalina. In the process of doing so, Anaconda Python (and thus the primary text processing engine used) was bricked because macOS refused to recognize developer-produced software in the root folder. Additionally, I lost the most recent work on the [Global_South_fall.rmd](https://github.com/nearridge/GlobalSouth/blob/master/Global_South_fall.md) file. That was quite tragic and I'm not entirely sure if something super important got lost, but I had a recent enough github backup so I was able to do some very creative downloading. 

```{r}
reticulate::py_discover_config(required_module="cleannlp")
```

Step two is to import the corpus that I previously produced by annotating every United Nations General Assembly speech from 1970 to the present day. That was using corpus_maker.r in roughly the manner described below. While I was working on this project, I stored the output of corpus_maker.r in a handful of different locations. First, I stored the full annotated corpus produced using cleanNLP on my desktop and on UChicago box. I store secondary versions that were produced using TidyText on GitHub. This was a rather inefficient method, but it's in the past. 

To produce the corpus, the following approximate code was run in corpus_maker.r:

```{r eval = FALSE}
imported_files <- read_tsv(here("Data", "raw_speeches_mikhaylov_project.tsv"))
full_annotation <- cnlp_annotate(imported_files, as_strings = TRUE)
all_words_annotation <- cnlp_get_token(full_annotation)
write_tsv(all_words_annotation, "../../../../../Desktop/full_un_corpus_annotated.tsv", na = "NA", col_names = TRUE)
```

What this does is it reads in the .tsv file downloaded from the [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y) that I store locally on my computer, then processes it through the cleanNLP annotation scheme using the spaCy backend. This produces analysis at several levels: tokens, sentences, and documents. For the purposes of this analysis, I only analyze the token level. I only stayed at this low-level of analysis for a few reasons. 

First, I am a novice programmer. I basically took what I was able to make work, and ran with it. Token-level analysis was the only thing that I was able to really get working on a large scale with the database. I corresponded with Andres Uribe [(uribe@uchicago.edu)](uribe@uchicago.edu), a PhD student in the Political Science department who is familiar with text analysis and R, to impliment analysis around `get_sentences()` but I wasn't able to make that happen.

Secondly, Professor Bradley is most interested in tracking vocabularies surrounding keywords. Vocabularies are made up of individual words, not sentences of full documents. As a result, the unit of communcation Professor Bradley is most interested in analyzing at this point is words, so I focus almost all my efforts on words alone. 

```{r cache = TRUE, cache.lazy = FALSE, include = FALSE}
annotated_imported_speeches <- read_tsv("../../../../../Desktop/full_un_corpus_annotated.tsv") %>%
  filter(!str_detect(word, "^[0-9]*$")) %>%
  filter(!upos == "PUNCT" & !upos == "SPACE") %>%
  mutate(lemma = strip(lemma)) %>%
  # correcting lemmas of some weird special cases. 
  # One day I might wordStem the entire thing. 
  mutate(lemma = if_else(lemma == "development", "develop", lemma)) %>%
  mutate(lemma = if_else(lemma == "government", "govern", lemma)) %>%
  mutate(id = str_sub(id, end=-5)) %>%
  separate(id, c("Country", "Session", "Year"), sep = "_", remove = FALSE)
all_words_only <- annotated_imported_speeches
all_words_only_1970 <- all_words_only %>% filter(Year %in% (1970:1979))
all_words_only_2k <- all_words_only %>% filter(Year %in% (2000:2009))
```

## Functions to determine the location of keywords, sentences around keywords, and distances between words

Next are three functions that are relevant to nuanced text analysis. 

First, I wrote a function to identify each sentence in each speech that contains a specific keyword. Even sentences that mention the keyword multiple times work. 

```{r}
# returns full tidytext df of sentences containing a specific keyword
sentences_with_keyword <- function(df, keyword) {
  returner <- df %>%
    filter(lemma == keyword) %>% 
    select(id, sid) %>% 
    distinct() %>% 
    left_join(df)
  return(returner)
}
```

Secondly, I wrote a function that extends that by getting the sentence before and after. This isn't perfect, but it works well enough. Sometimes duplicates or non-sentences are created but on large scales, this is trivial. 

```{r}
# This grabs the sentence in front of, the sentence itself, and the sentence behind the keyword. 
local_sentences_with_keyword <- function(df, keyword) {
  sentence_ids <- df %>%
    filter(lemma == keyword) %>% 
    select(id, sid) %>% 
    distinct()
  all_ids_sids <- all_words_only %>% 
    select(id, sid) %>% 
    distinct() 
  #need to lag it to the sid before and lead it to the sid after. Can't just use +1 and -1 because of weirdness in how data got processed. 
  before_middle_after_sentence_ids <- bind_rows(sentence_ids, sentence_ids %>% mutate(sid = sid - 1), sentence_ids %>% mutate(sid = sid + 1)) %>%
    distinct() %>%
    arrange(id, sid)
  returner <- semi_join(df, before_middle_after_sentence_ids) %>%
    arrange(id, sid)
  return(returner)
}
```

Finally, I wrote a function that finds out the distance between two words in a string. It is my baby. It's important because my (read: Neeraj's possibly misinformed) hypothesis is that words relevant to a keyword will be said closer in a sentence to the keyword itself. At minimum, important words will be mentioned a lot. The function is pretty complicated but well coded and if a keyword is mentioned multiple times, the distance from a word to the keyword is minimized. 

```{r}
# Builds a function that counts how far apart two words are.
# the df will almost allways be all_words_only
distance_between_words <- function(df, keyword) {
  sentence_length_of_all_sentences <- df %>%
    group_by(id, sid) %>%
    filter(row_number(tid) == n()) %>%
    select(id, sid, slength = tid)
  
  # This grabs all sentences with the keyword. It produces duplicates if the keyword appears multiple times. Those are removed later on.
  df_of_location_sentences_with_keyword <- df %>%
    filter(lemma == keyword) %>%
    select(id, sid) %>%
    mutate(sentence_number = row_number()) %>%
    unite(cat, c(id, sid), remove = FALSE) %>%
    mutate(is_duplicate = duplicated(cat)) %>%
    mutate(is_duplicate = if_else(is_duplicate == TRUE, TRUE, duplicated(cat, fromLast = TRUE))) %>%
    left_join(sentence_length_of_all_sentences)
  
  df_of_all_sentences_with_keyword <- inner_join(df, df_of_location_sentences_with_keyword) %>% select(id, sid, cat, tid, slength, word, lemma, sentence_number, is_duplicate)
  # # Begin distance_between_words
  sentences_containing_keyword <- df_of_all_sentences_with_keyword
  
  location_of_keywords_in_each_sentence <- sentences_containing_keyword %>%
    mutate(keyword_row_number = if_else(lemma == keyword, location <- tid, 0)) %>%
    filter(lemma == keyword) %>%
    select(sentence_number, keyword_row_number)
  export <- inner_join(sentences_containing_keyword, location_of_keywords_in_each_sentence, by = ("sentence_number")) %>%
    unite(cat, c(cat, tid), remove = FALSE) %>%
    mutate(distance_between_word_and_keyword = abs(keyword_row_number - tid)) %>%
    mutate(compare_value = if_else(is_duplicate, lead(distance_between_word_and_keyword), 0)) %>%
    distinct(cat, .keep_all = TRUE) %>%
    mutate(distance_between_word_and_keyword = if_else(is_duplicate, pmin(distance_between_word_and_keyword, compare_value), distance_between_word_and_keyword)) %>%
    mutate(distance_from_keyword_percent = distance_between_word_and_keyword/slength)
  return(export)
}
```

# Analysis

When we first started out in the Fall, we thought that the words we were most interested in were:

* Govern-
* Develop-
* Secur-

and any relevant derivatives of them. Those are words that I tagged when they appeared in a speech. I need to update the next three sections of code to reflect the current function that gets distances between words and abilities to do the entire timespan of the corpus; not just 1970 to 1979. 

## Mentions of Govern with related words form the 1970 to 1979 corpus

```{r}
distance_between_govern_other_words <- distance_between_words(all_words_only_1970, "govern") %>%
  select(id, sid, word, lemma, slength, distance_between_word_and_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(27:n()) %>%
  arrange(-count) %>%
  filter(lemma != "govern")
ggplot(distance_between_govern_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Govern) versus mentions")
# related words: scatter plot with count of total times said on x axis and distance from govern on y axis
# for some reason, the sentence division within the annotated_imported_files object is empty so cnlp_get_sentences returns an empty dataframe. What do I need to configure differently to get a) sentences to be parsed by the cnlp_annotate function up here and b) extract actual sentences and any information at that level.
kable(distance_between_govern_other_words %>% slice(1:15))
```


## Mentions of Secure with related words form the 1970 to 1979 corpus

```{r}
distance_between_secure_other_words <- distance_between_words(all_words_only_1970, "secure") %>%
  select(id, sid, word, lemma, slength, distance_between_word_and_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(24:n()) %>%
  arrange(-count) %>%
  filter(lemma != "secure")
ggplot(distance_between_secure_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Secure) versus mentions")
kable(distance_between_secure_other_words %>% slice(1:15))
```

## Mentions of Develop with related words form the 1970 to 1979 corpus

```{r}
distance_between_develop_other_words <- distance_between_words(all_words_only_1970, "develop") %>%
  select(id, sid, word, lemma, slength, distance_between_word_and_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(215:n()) %>%
  arrange(-count) %>%
  filter(lemma != "develop")
ggplot(distance_between_develop_other_words, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions")
# that thing sitting all alone at 21 k is "country" unsurprisingly
ggplot(distance_between_develop_other_words %>% filter(lemma != "country"), mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions (minus country)")
kable(distance_between_develop_other_words %>% slice(1:15))
```

We had previously only looked at 1970 to 1979. Develop is super meaty, so we'll try to do some analysis on stuff way after 1979. We should expect to see some significant differences in the language of say, the 2000s compared to the language of the 1970s. 

Lets get an annotated corpus over 2000 to 2009. 

ITS GONE NOW AND HAS MOVED WAY UP TO THE START SETUP

Now lets look at develop and compare it to 1970s stuff. Especially in the context of a specific nation, say Indonesia. 

```{r}
distance_between_develop_other_words_2k <- distance_between_words(all_words_only_2k, "develop") %>%
  select(id, sid, word, lemma, slength, distance_between_word_and_keyword, distance_from_keyword_percent) %>%
  anti_join(stop_words) %>%
  group_by(lemma) %>%
  summarise(count = n(), mean(distance_from_keyword_percent)) %>%
  slice(87:n()) %>%
  arrange(-count) %>%
  filter(lemma != "develop")
ggplot(distance_between_develop_other_words_2k, mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions in 2000s")
# that thing sitting all alone at 8.8 k is "country" unsurprisingly
ggplot(distance_between_develop_other_words_2k %>% filter(lemma != "country"), mapping = aes(`count`, `mean(distance_from_keyword_percent)`)) +
  geom_point() +
  labs(x = "Number of Mentions of Word", y = "Average Distance from Keyword", title = "Relative distance from relative keyword (Develop) versus mentions (minus country) in 2000s")
kable(distance_between_develop_other_words_2k %>% slice(1:15))
```


# TFIDF

The most recent breakthrough has been on TFIDF. 

```{r}
#List of countries for filtering
country_filter <- str_to_lower(pull(codelist, country.name.en)) 
# I DONT WANT TO STEM RIGHT NOW. THAT MEANS RIGHT AND RIGHTS ARE DIFFERENT THINGS
# Brute force adds stemming but this is kinda unnecessary because lemma works well enough, at least so far.
#all_words_only <- all_words_only %>%
#  mutate(word_stem = wordStem(lemma))
# Takes in a df of the words that appear in a sentence containing a keyword and returns a df of words that appear within a certain range of years. 
# Common ranges of years: a decade, each year, five years. 
# Example question this function answers: What were the most important words to govern in a 5 year period?
tfidfer <- function(df, start_year, up_to) {
  tfidf_over_range <- df %>%
    filter(Year %in% seq(start_year, start_year + up_to)) %>%
    bind_tf_idf(lemma, id, n) %>%
    filter(lemma %in% country_filter == FALSE) %>%
    anti_join(stop_words, by = c("lemma" = "word"))
  return(tfidf_over_range)
}
# Function that gets the top tfidf words in each year. This is applied to a dataframe that already has done tfidf analyis on a bunch of words related to a keyword over a period of time.
# The keyword technically does not matter for this function. 
tfidf_aux_word_organizer <- function(df_of_tfidf_of_every_individual_word) {
  df_of_tfidf_of_every_individual_word <- df_of_tfidf_of_every_individual_word %>% 
    separate(id, c("Country", "Session", "Year")) %>%
    group_by(Year, lemma) %>%
    summarize(mean_tfidf = mean(tf_idf), n = n()) %>%
    filter(n >= 5) %>%
    ungroup() %>%
    mutate(ratio = mean_tfidf/n) %>%
    arrange(desc(ratio)) %>%
    filter(lemma != "develop" 
           & lemma != "pron" 
           & lemma != "people" 
           & lemma != ""
           & lemma != "country"
           & lemma != "countries"
           & lemma != ""
           & lemma != "united"
           & lemma != "nation"
           & lemma != "nations"
           & lemma != "international"
           & lemma != "'s") %>% group_by(Year) %>% top_n(n = 10) 
  return(df_of_tfidf_of_every_individual_word)
}
printer_producer <- function(dataframe) {
  dataframe <- dataframe %>% 
    arrange(-desc(Year))
  data1 <- list()
  data2 <- list()
  for (i in seq(as.double(min(dataframe$Year)), as.double(max(dataframe$Year)) - 5)) {
    data1 <- bind_cols(data1, dataframe %>% filter(Year == i))
  }
  for (j in seq(as.double(max(dataframe$Year)) - 4, as.double(max(dataframe$Year)))) {
    data2 <- bind_cols(data2, dataframe %>% filter(Year == j))
  }
  return(bind_rows(data1, data2))
}

exceler <- function(dataframe) {
  dataframe <- dataframe %>% 
    arrange(-desc(Year))
  data1 <- list()
  data2 <- list()
  data3 <- list()
  data4 <- list()
  for (i in seq(as.double(min(dataframe$Year)), as.double(max(dataframe$Year)) - 7)) {
    data1 <- bind_cols(data1, dataframe %>% filter(Year == i))
  }
  for (j in seq(as.double(max(dataframe$Year)) - 6, as.double(max(dataframe$Year)) - 4)) {
    data2 <- bind_cols(data2, dataframe %>% filter(Year == j))
  }
  for (k in seq(as.double(max(dataframe$Year)) - 3, as.double(max(dataframe$Year)) - 1)) {
    data3 <- bind_cols(data3, dataframe %>% filter(Year == k))
  }
  data4 <- bind_cols(data4, dataframe %>% filter(Year == as.double(max(dataframe$Year))))
  
  return(bind_rows(data1, data2, data3, data4))
}
```

## TFIDF for stuff related to govern. 

Single sentence.

```{r}
# note that this produces an unstemmed all_words_only. 
sentences_mention_govern <- sentences_with_keyword(all_words_only, "govern")
num_words_in_each_speech_mentioning_govern <- sentences_mention_govern %>% 
  count(id, lemma) %>% 
  group_by(id) %>%
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_sentences_containing_govern <- left_join(sentences_mention_govern %>% count(id, lemma), num_words_in_each_speech_mentioning_govern) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
# tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1970, 9) %>% select(-Year, -Session, -Country) is equal to govern_full_tfidf_1970_ided
# furthermore, all of the govern_xxxxs equals the previous govern_full_tfidf_xxxx_yeared I previously had. Thus, it is the proper implimentation. 
govern_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1970, 9) %>% select(-Year, -Session, -Country))
govern_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1980, 9) %>% select(-Year, -Session, -Country))
govern_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 1990, 9) %>% select(-Year, -Session, -Country))
govern_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 2000, 9) %>% select(-Year, -Session, -Country))
govern_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_govern, 2010, 9) %>% select(-Year, -Session, -Country))

govern_single_alltime <- bind_rows(printer_producer(govern_1970s), printer_producer(govern_1980s), printer_producer(govern_1990s), printer_producer(govern_2000s), printer_producer(govern_2010s))

excelTable(govern_single_alltime, editable = FALSE, pagination = 20)
```

Local Sentences included. 

```{r}
local_sentences_mention_govern <- local_sentences_with_keyword(all_words_only, "govern")
num_words_insentences_locally_mentioning_govern <- local_sentences_mention_govern %>% 
  count(id, lemma) %>% 
  group_by(id) %>%
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_localsentences_containing_govern <- left_join(local_sentences_mention_govern %>% count(id, lemma), num_words_insentences_locally_mentioning_govern) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
extended_govern_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_govern, 1970, 9) %>% select(-Year, -Session, -Country))
extended_govern_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_govern, 1980, 9) %>% select(-Year, -Session, -Country))
extended_govern_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_govern, 1990, 9) %>% select(-Year, -Session, -Country))
extended_govern_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_govern, 2000, 9) %>% select(-Year, -Session, -Country))
extended_govern_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_govern, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(extended_govern_1970s), printer_producer(extended_govern_1980s), printer_producer(extended_govern_1990s), printer_producer(extended_govern_2000s), printer_producer(extended_govern_2010s)))

govern_local_alltime <- bind_rows(printer_producer(extended_govern_1970s),printer_producer(extended_govern_1980s),printer_producer(extended_govern_1990s),
                     printer_producer(extended_govern_2000s),printer_producer(extended_govern_2010s))

excelTable(govern_local_alltime, editable = FALSE, pagination = 20)
```

## TFIDF for stuff related to Develop

Single sentence. 

```{r}
# note that this produces an unstemmed all_words_only. 
sentences_mention_develop <- sentences_with_keyword(all_words_only, "develop")
num_words_in_each_speech_mentioning_develop <- sentences_mention_develop %>% 
  count(id, lemma) %>% 
  group_by(id) %>% 
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_sentences_containing_develop <- left_join(sentences_mention_develop %>% count(id, lemma), num_words_in_each_speech_mentioning_develop) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
develop_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 1970, 9) %>% select(-Year, -Session, -Country))
develop_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 1980, 9) %>% select(-Year, -Session, -Country))
develop_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 1990, 9) %>% select(-Year, -Session, -Country))
develop_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 2000, 9) %>% select(-Year, -Session, -Country))
develop_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_develop, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(develop_1970s %>% filter(lemma != "lucia")), printer_producer(develop_1980s), printer_producer(develop_1990s), printer_producer(develop_2000s),printer_producer(develop_2010s)))
```

Local sentences included. 

```{r}
local_sentences_mention_develop <- local_sentences_with_keyword(all_words_only, "develop")
num_words_insentences_locally_mentioning_develop <- local_sentences_mention_develop %>% 
  count(id, lemma) %>% 
  group_by(id) %>%
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_localsentences_containing_develop <- left_join(local_sentences_mention_develop %>% count(id, lemma), num_words_insentences_locally_mentioning_develop) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
extended_develop_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_develop, 1970, 9) %>% select(-Year, -Session, -Country))
extended_develop_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_develop, 1980, 9) %>% select(-Year, -Session, -Country))
extended_develop_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_develop, 1990, 9) %>% select(-Year, -Session, -Country))
extended_develop_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_develop, 2000, 9) %>% select(-Year, -Session, -Country))
extended_develop_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_develop, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(extended_develop_1970s), printer_producer(extended_develop_1980s), printer_producer(extended_develop_1990s), printer_producer(extended_develop_2000s), printer_producer(extended_develop_2010s)))
```

## TFIDF for stuff related to Rights

Single sentence. 

```{r}
# "rights" gets lemma'ed to "right"
# "Rights" gets lemma'ed to "rights." I do some overriding to get them to be the same in this context.
sentences_mention_right <- sentences_with_keyword(all_words_only %>% mutate(lemma = if_else(lemma == "rights", "right", lemma)), "right")
num_words_in_each_speech_mentioning_right <- sentences_mention_right %>%  
  count(id, lemma) %>% 
  group_by(id) %>% 
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_sentences_containing_right <- left_join(sentences_mention_right %>% count(id, lemma), num_words_in_each_speech_mentioning_right) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
right_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 1970, 9) %>% select(-Year, -Session, -Country))
right_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 1980, 9) %>% select(-Year, -Session, -Country))
right_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 1990, 9) %>% select(-Year, -Session, -Country))
right_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 2000, 9) %>% select(-Year, -Session, -Country))
right_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_right, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(right_1970s), printer_producer(right_1980s), printer_producer(right_1990s), printer_producer(right_2000s),printer_producer(right_2010s)))
```

Local sentences included. 

```{r}
local_sentences_mention_right <- local_sentences_with_keyword(all_words_only %>% mutate(lemma = if_else(lemma == "rights", "right", lemma)), "right")
num_words_insentences_locally_mentioning_right <- local_sentences_mention_right %>% 
  count(id, lemma) %>% 
  group_by(id) %>%
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_localsentences_containing_right <- left_join(local_sentences_mention_right %>% count(id, lemma), num_words_insentences_locally_mentioning_right) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
extended_right_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_right, 1970, 9) %>% select(-Year, -Session, -Country))
extended_right_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_right, 1980, 9) %>% select(-Year, -Session, -Country))
extended_right_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_right, 1990, 9) %>% select(-Year, -Session, -Country))
extended_right_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_right, 2000, 9) %>% select(-Year, -Session, -Country))
extended_right_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_right, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(extended_right_1970s), printer_producer(extended_right_1980s), printer_producer(extended_right_1990s), printer_producer(extended_right_2000s), printer_producer(extended_right_2010s)))
```

## TFIDF for stuff related to capability

Relevant Words:

* capability, mentioned 1674 times
* capable, mentioned 1933 times
* capacity, mentioned 5748 times
* ability, mentioned 3239 times. I do not include this in my calculation however. 

Single sentence. 

```{r}
sentences_mention_capability <- sentences_with_keyword(all_words_only %>% mutate(lemma = if_else(lemma == "capable", "capability", lemma)) %>% mutate(lemma = if_else(lemma == "capacity", "capability", lemma)), "capability")
num_words_in_each_speech_mentioning_capability <- sentences_mention_capability %>%  
  count(id, lemma) %>% 
  group_by(id) %>% 
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_sentences_containing_capability <- left_join(sentences_mention_capability %>% count(id, lemma), num_words_in_each_speech_mentioning_capability) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
capability_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_capability, 1970, 9) %>% select(-Year, -Session, -Country))
capability_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_capability, 1980, 9) %>% select(-Year, -Session, -Country))
capability_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_capability, 1990, 9) %>% select(-Year, -Session, -Country))
capability_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_capability, 2000, 9) %>% select(-Year, -Session, -Country))
capability_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_capability, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(capability_1970s), printer_producer(capability_1980s), printer_producer(capability_1990s), printer_producer(capability_2000s),printer_producer(capability_2010s)))
```

Local sentences included. 

```{r}
local_sentences_mention_capability <- local_sentences_with_keyword(all_words_only %>% mutate(lemma = if_else(lemma == "capable", "capability", lemma)) %>% mutate(lemma = if_else(lemma == "capacity", "capability", lemma)), "capability")
num_words_insentences_locally_mentioning_capability <- local_sentences_mention_capability %>% 
  count(id, lemma) %>% 
  group_by(id) %>%
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_localsentences_containing_capability <- left_join(local_sentences_mention_capability %>% count(id, lemma), num_words_insentences_locally_mentioning_capability) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
extended_capability_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_capability, 1970, 9) %>% select(-Year, -Session, -Country))
extended_capability_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_capability, 1980, 9) %>% select(-Year, -Session, -Country))
extended_capability_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_capability, 1990, 9) %>% select(-Year, -Session, -Country))
extended_capability_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_capability, 2000, 9) %>% select(-Year, -Session, -Country))
extended_capability_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_localsentences_containing_capability, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(extended_capability_1970s), printer_producer(extended_capability_1980s), printer_producer(extended_capability_1990s), printer_producer(extended_capability_2000s), printer_producer(extended_capability_2010s)))
```

in comparison:

* right, mentioned 48186 times
* develop, mentioned 113759 times
* govern, 47939

I found this out by running the following code with view around it and without slice. I found the counts of words using the command f function. 

```{r}
all_words_only %>% count(lemma) %>% arrange(desc(n)) %>% anti_join(stop_words, by = c("lemma" = "word")) %>% slice(1:10)
```

To check in the future. Look at these for just one sentence. Lemmas

* responsibility, mentions 14k 
* dialogue, 11k
* **sustainable	, 11k**
* **solidarity, 8k**
* **resilience + resilient + resiliency combined.** 

```{r}
sentences_mention_responsibility <- sentences_with_keyword(all_words_only %>% mutate(lemma = if_else(lemma == "responsible", "responsibility", lemma)), "responsibility")
num_words_in_each_speech_mentioning_responsibility <- sentences_mention_responsibility %>%  
  count(id, lemma) %>% 
  group_by(id) %>% 
  summarize(`words in speech` = sum(n))
mentions_of_aux_words_in_sentences_containing_responsibility <- left_join(sentences_mention_responsibility %>% count(id, lemma), num_words_in_each_speech_mentioning_responsibility) %>%
  separate(id, c("Country", "Session", "Year"), remove = FALSE)
responsibility_1970s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_responsibility, 1970, 9) %>% select(-Year, -Session, -Country))
responsibility_1980s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_responsibility, 1980, 9) %>% select(-Year, -Session, -Country))
responsibility_1990s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_responsibility, 1990, 9) %>% select(-Year, -Session, -Country))
responsibility_2000s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_responsibility, 2000, 9) %>% select(-Year, -Session, -Country))
responsibility_2010s <- tfidf_aux_word_organizer(tfidfer(mentions_of_aux_words_in_sentences_containing_responsibility, 2010, 9) %>% select(-Year, -Session, -Country))
#View(bind_rows(printer_producer(capability_1970s), printer_producer(capability_1980s), printer_producer(capability_1990s), printer_producer(capability_2000s),printer_producer(capability_2010s)))

responsibility_alltime <- bind_rows(printer_producer(responsibility_1970s), printer_producer(responsibility_1980s), printer_producer(responsibility_1990s), printer_producer(responsibility_2000s), printer_producer(responsibility_2010s))

excelTable(responsibility_alltime, editable = FALSE, pagination = 20)
```



Feb 5 2020
We need to think about extra sentences as going farther out in terms of accuracy of the valance key words. Single sentence gives us the core language surrounding the keyword. If you were to argue the greater sentences, you'd argue that they are less convincing but still substantive. 


Govern, capability are working
capability is the only one that had really useful insight on extra sentences. 

Development is working alright in showing a difference. 

Rights didnt work well for us period. It's not helpful.

The single sentence was fundamental and core. The extended sentences will function as a helper context. 


rerun the big chunk above for rights and capibailities
adjust the words from funciton to look at sentence before and after especially for govern_1970s
